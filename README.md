ID	Title					BibTex					Input		Output		Summary				
1	3D Object Detection with Pointformer					"@INPROCEEDINGS{9578669,
  author={Pan, Xuran and Xia, Zhuofan and Song, Shiji and Li, Li Erran and Huang, Gao},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={3D Object Detection with Pointformer}, 
  year={2021},
  volume={},
  number={},
  pages={7459-7468},
  doi={10.1109/CVPR46437.2021.00738}
}"					point cloud		classified labels, segmented images, etc.		"1. A transformer based U-Net shaped model
2. Local transformer, global transformer, local-global transformer
3. Coordinate refinement
4. Positional encoding
5. Computational cost reduction - Linformer"				
																			
																			
2	Point Transformer					"@INPROCEEDINGS{9710703,
  author={Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip and Koltun, Vladlen},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Point Transformer}, 
  year={2021},
  volume={},
  number={},
  pages={16239-16248},
  doi={10.1109/ICCV48922.2021.01595}
}"					point cloud		classified labels, segmented images, etc.		"1. A transformer based U-Net shaped network for semantic segmentation and a transformer based feed-forward network for classification
2. Point transformer block, transition down block, transition up block
3. Positional encoding
"				
																			
																			
3	PCT: Point cloud transformer					"@ARTICLE{2020arXiv201209688G,
       author = {{Guo}, Meng-Hao and {Cai}, Jun-Xiong and {Liu}, Zheng-Ning and {Mu}, Tai-Jiang and {Martin}, Ralph R. and {Hu}, Shi-Min},
        title = ""{PCT: Point cloud transformer}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2020,
        month = dec,
          eid = {arXiv:2012.09688},
        pages = {arXiv:2012.09688},
          doi = {10.48550/arXiv.2012.09688},
archivePrefix = {arXiv},
       eprint = {2012.09688},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201209688G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		classified labels, segmented images, etc.		"1. A transformer based network for semantic segmentation and classification
2. Positional encoding and input embedding
3. Offset attention
4. Neighbor embedding
"				
																			
																			
4	Point Transformer					"@ARTICLE{2020arXiv201100931E,
       author = {{Engel}, Nico and {Belagiannis}, Vasileios and {Dietmayer}, Klaus},
        title = ""{Point Transformer}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2020,
        month = nov,
          eid = {arXiv:2011.00931},
        pages = {arXiv:2011.00931},
          doi = {10.48550/arXiv.2011.00931},
archivePrefix = {arXiv},
       eprint = {2011.00931},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201100931E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		classified labels, segmented images, etc.		"1. A transformer based network for semantic segmentation and classification
2. Local feature generator, global feature generator and local-global attention
3. SortNet (contained in the local feature generator)
"				
																			
																			
5	PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers					"@INPROCEEDINGS{9710121,
  author={Yu, Xumin and Rao, Yongming and Wang, Ziyi and Liu, Zuyan and Lu, Jiwen and Zhou, Jie},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={12478-12487},
  doi={10.1109/ICCV48922.2021.01227}}
"					point cloud		classified labels, segmented images, etc.		"1. A transformer based network for point cloud completion
2. Point proxy (a vector representation of points)
3. Geometry-aware transformer block
4. Multi-Scale Point Cloud Generation"				
																			
																			
6	Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling					"@INPROCEEDINGS{8954194,
  author={Yang, Jiancheng and Zhang, Qiang and Ni, Bingbing and Li, Linguo and Liu, Jinxian and Zhou, Mengdie and Tian, Qi},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Modeling Point Clouds With Self-Attention and Gumbel Subset Sampling}, 
  year={2019},
  volume={},
  number={},
  pages={3318-3327},
  doi={10.1109/CVPR.2019.00344}}
"					point cloud		classified labels, segmented images, etc.		"1. A transformer based hierarchical network for segmentation and classification
2. Absolute and relative position embedding
3. Gumbel subset sampling
4. Group shuffle attention module"				
																			
																			
7	Dynamic Clustering Transformer Network for Point Cloud Segmentation					"@ARTICLE{2023arXiv230608073L,
       author = {{Lu}, Dening and {Zhou}, Jun and {Yilin Gao}, Kyle and {Li}, Dilong and {Du}, Jing and {Xu}, Linlin and {Li}, Jonathan},
        title = ""{Dynamic Clustering Transformer Network for Point Cloud Segmentation}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = may,
          eid = {arXiv:2306.08073},
        pages = {arXiv:2306.08073},
          doi = {10.48550/arXiv.2306.08073},
archivePrefix = {arXiv},
       eprint = {2306.08073},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230608073L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"													
																			
																			
8	"Collect-and-Distribute Transformer for
3D Point Cloud Analysis"					"@ARTICLE{2023arXiv230601257Q,
       author = {{Qiu}, Haibo and {Yu}, Baosheng and {Tao}, Dacheng},
        title = ""{Collect-and-Distribute Transformer for 3D Point Cloud Analysis}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.01257},
        pages = {arXiv:2306.01257},
          doi = {10.48550/arXiv.2306.01257},
archivePrefix = {arXiv},
       eprint = {2306.01257},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230601257Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"													
																			
																			
9	"GTNet: Graph Transformer Network for 3D Point
Cloud Classification and Semantic Segmentation"					"@ARTICLE{2023arXiv230515213Z,
       author = {{Zhou}, Wei and {Wang}, Qian and {Jin}, Weiwei and {Shi}, Xinzhe and {He}, Ying},
        title = ""{GTNet: Graph Transformer Network for 3D Point Cloud Classification and Semantic Segmentation}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = may,
          eid = {arXiv:2305.15213},
        pages = {arXiv:2305.15213},
          doi = {10.48550/arXiv.2305.15213},
archivePrefix = {arXiv},
       eprint = {2305.15213},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230515213Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"													
																			
																			
10	Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification					"@ARTICLE{2023arXiv230405694W,
       author = {{Wei}, Xian and {Wang}, Muyu and {Lin}, Shing-Ho Jonathan and {Li}, Zhengyu and {Yang}, Jian and {Al-Jawari}, Arafat and {Tang}, Xuan},
        title = ""{Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = apr,
          eid = {arXiv:2304.05694},
        pages = {arXiv:2304.05694},
          doi = {10.48550/arXiv.2304.05694},
archivePrefix = {arXiv},
       eprint = {2304.05694},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230405694W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"													
																			
																			
11	PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation					"@INPROCEEDINGS{8099499,
  author={Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}, 
  year={2017},
  volume={},
  number={},
  pages={77-85},
  doi={10.1109/CVPR.2017.16}}
"					point cloud		classified labels, segmented images, etc.		A CNN based permutation invariant network for segmentation and classification				
																			
																			
12	"PointNet++: Deep Hierarchical Feature Learning on
Point Sets in a Metric Space"					"@ARTICLE{2017arXiv170602413Q,
       author = {{Qi}, Charles R. and {Yi}, Li and {Su}, Hao and {Guibas}, Leonidas J.},
        title = ""{PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.02413},
        pages = {arXiv:1706.02413},
          doi = {10.48550/arXiv.1706.02413},
archivePrefix = {arXiv},
       eprint = {1706.02413},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170602413Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					point cloud		classified labels, segmented images, etc.		"1. A CNN based hierarchical network for segmentation and classification
2. Set abstraction level (sampling layer + grouping layer + PointNet layer)
3. Density adaptive PointNet layer (Multi-scale grouping and multi-resolution grouping)
4. Point feature propagation for segmentation (upsampling)
"				
																			
																			
13 *	"TransGrasp: A Multi-Scale Hierarchical Point Transformer
for 7-DoF Grasp Detection"					"@INPROCEEDINGS{9812001,
  author={Liu, Zhixuan and Chen, Zibo and Xie, Shangjin and Zheng, Wei–Shi},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)}, 
  title={TransGrasp: A Multi-Scale Hierarchical Point Transformer for 7-DoF Grasp Detection}, 
  year={2022},
  volume={},
  number={},
  pages={1533-1539},
  doi={10.1109/ICRA46639.2022.9812001}}
"					point cloud		7-DoF grasp pose		"1. A transformer based network for 7-DoF grasp pose prediction
2. A novel grasp pose definition
3. Multi-scale local-global transformer block and multi-scale local geometry representation (encoder)
4. Co-attention up-sampling block (decoder)
5. A bin-based regression method to obtain the grasp pose orientation
6. A new loss function"				
																			
																			
14	TransSC: Transformer-based Shape Completion for Grasp Evaluation					"@ARTICLE{2021arXiv210700511C,
       author = {{Chen}, Wenkai and {Liang}, Hongzhuo and {Chen}, Zhaopeng and {Sun}, Fuchun and {Zhang}, Jianwei},
        title = ""{TransSC: Transformer-based Shape Completion for Grasp Evaluation}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2021,
        month = jul,
          eid = {arXiv:2107.00511},
        pages = {arXiv:2107.00511},
          doi = {10.48550/arXiv.2107.00511},
archivePrefix = {arXiv},
       eprint = {2107.00511},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210700511C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					point cloud		point cloud		"1. A transformer and CNN based network for point cloud completion
2. A dataset of partial point cloud"				
																			
																			
15 *	"Contact-GraspNet: Efficient 6-DoF Grasp Generation
in Cluttered Scenes"					"@INPROCEEDINGS{9561877,
  author={Sundermeyer, Martin and Mousavian, Arsalan and Triebel, Rudolph and Fox, Dieter},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes}, 
  year={2021},
  volume={},
  number={},
  pages={13438-13444},
  doi={10.1109/ICRA48506.2021.9561877}}
"					rgb-d and point cloud		7-DoF grasp pose		"1. A CNN based network for 7-DoF grasp pose prediction
2. A novel grasp pose definition
3. A method of projecting the ground truth 6-DoF grasps on the recorded point cloud
4. A new loss function"				
																			
																			
16 *	"GraspNet-1Billion: A Large-Scale Benchmark
for General Object Grasping"					"@INPROCEEDINGS{9156992,
  author={Fang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping}, 
  year={2020},
  volume={},
  number={},
  pages={11441-11450},
  doi={10.1109/CVPR42600.2020.01146}}
"					point cloud		7-DoF grasp pose, grasp confidence		"1. A CNN based network for grasp pose prediction
2. A large dataset of graspable objects
3. A novel grasp pose definition
4. A new loss function
5. A new evaluation metric"				
																			
																			
17	"PointNet++ Grasping: Learning An End-to-end Spatial Grasp 
Generation Algorithm from Sparse Point Clouds"					"@ARTICLE{2020arXiv200309644N,
       author = {{Ni}, Peiyuan and {Zhang}, Wenguang and {Zhu}, Xiaoxiao and {Cao}, Qixin},
        title = ""{PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2020,
        month = mar,
          eid = {arXiv:2003.09644},
        pages = {arXiv:2003.09644},
          doi = {10.48550/arXiv.2003.09644},
archivePrefix = {arXiv},
       eprint = {2003.09644},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200309644N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					point cloud		grasp pose, grasp quality		"1. A CNN based network for grasp pose prediction
2. A dataset of single-object scenes and multi-objects scenes
3. A metric for grasp quality evaluation
4. A new loss function"				
																			
																			
18	PointNetGPD: Detecting Grasp Configurations from Point Sets					"@INPROCEEDINGS{8794435,
  author={Liang, Hongzhuo and Ma, Xiaojian and Li, Shuang and Görner, Michael and Tang, Song and Fang, Bin and Sun, Fuchun and Zhang, Jianwei},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={PointNetGPD: Detecting Grasp Configurations from Point Sets}, 
  year={2019},
  volume={},
  number={},
  pages={3629-3635},
  doi={10.1109/ICRA.2019.8794435}}
"					point cloud		classified labels		"1. A CNN based network for grasp quality evaluation
2. A large dataset of graspable objects
3. A metric for grasp quality evaluation"				
																			
																			
19	"REGNet: REgion-based Grasp Network for End-to-end Grasp
Detection in Point Clouds"					"@INPROCEEDINGS{9561920,
  author={Zhao, Binglei and Zhang, Hanbo and Lan, Xuguang and Wang, Haoyu and Tian, Zhiqiang and Zheng, Nanning},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={REGNet: REgion-based Grasp Network for End-to-end Grasp Detection in Point Clouds}, 
  year={2021},
  volume={},
  number={},
  pages={13474-13480},
  doi={10.1109/ICRA48506.2021.9561920}}
"					point cloud		7-DoF grasp pose		"1. A CNN based network for grasp pose prediction
2. A large dataset of graspable objects
3. Grasp quality score and point grasp confidence
4. Grasp region and grasp anchor mechanism
5. A new loss function"				
																			
																			
20 *	"S4G: Amodal Single-view Single-Shot SE(3) Grasp
Detection in Cluttered Scenes"					"@ARTICLE{2019arXiv191014218Q,
       author = {{Qin}, Yuzhe and {Chen}, Rui and {Zhu}, Hao and {Song}, Meng and {Xu}, Jing and {Su}, Hao},
        title = ""{S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = oct,
          eid = {arXiv:1910.14218},
        pages = {arXiv:1910.14218},
          doi = {10.48550/arXiv.1910.14218},
archivePrefix = {arXiv},
       eprint = {1910.14218},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191014218Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					point cloud		6-DoF pose, grasp quality score		"1. A CNN based network for grasp pose prediction
2. A large dataset of graspable objects in simulated scenes
3. Some scores for grasp quality evaluation
4. A gripper contact model
5. A new grasp representation
6. A new loss function"				
																			
																			
21	Attention Is All You Need					"@ARTICLE{2017arXiv170603762V,
       author = {{Vaswani}, Ashish and {Shazeer}, Noam and {Parmar}, Niki and {Uszkoreit}, Jakob and {Jones}, Llion and {Gomez}, Aidan N. and {Kaiser}, Lukasz and {Polosukhin}, Illia},
        title = ""{Attention Is All You Need}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.03762},
        pages = {arXiv:1706.03762},
          doi = {10.48550/arXiv.1706.03762},
archivePrefix = {arXiv},
       eprint = {1706.03762},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603762V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					text		text		An architecture for NLP				
																			
																			
22	An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale					"@ARTICLE{2020arXiv201011929D,
       author = {{Dosovitskiy}, Alexey and {Beyer}, Lucas and {Kolesnikov}, Alexander and {Weissenborn}, Dirk and {Zhai}, Xiaohua and {Unterthiner}, Thomas and {Dehghani}, Mostafa and {Minderer}, Matthias and {Heigold}, Georg and {Gelly}, Sylvain and {Uszkoreit}, Jakob and {Houlsby}, Neil},
        title = ""{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.11929},
        pages = {arXiv:2010.11929},
          doi = {10.48550/arXiv.2010.11929},
archivePrefix = {arXiv},
       eprint = {2010.11929},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201011929D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					rgb		classified labels, segmented images, etc.		A transformer based network for CV				
																			
																			
23	Fastformer: Additive Attention Can Be All You Need					"@ARTICLE{2021arXiv210809084W,
       author = {{Wu}, Chuhan and {Wu}, Fangzhao and {Qi}, Tao and {Huang}, Yongfeng and {Xie}, Xing},
        title = ""{Fastformer: Additive Attention Can Be All You Need}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2021,
        month = aug,
          eid = {arXiv:2108.09084},
        pages = {arXiv:2108.09084},
          doi = {10.48550/arXiv.2108.09084},
archivePrefix = {arXiv},
       eprint = {2108.09084},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210809084W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					text		text		A more computationally efficient transformer architecture				
																			
																			
24	Linformer: Self-attention with linear complexity					"@ARTICLE{2020arXiv200604768W,
       author = {{Wang}, Sinong and {Li}, Belinda Z. and {Khabsa}, Madian and {Fang}, Han and {Ma}, Hao},
        title = ""{Linformer: Self-Attention with Linear Complexity}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.04768},
        pages = {arXiv:2006.04768},
          doi = {10.48550/arXiv.2006.04768},
archivePrefix = {arXiv},
       eprint = {2006.04768},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200604768W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					text		text		A more computationally efficient transformer architecture				
																			
																			
25	Spatial Transformer Networks					"@article{2015Spatial,
  title={Spatial Transformer Networks},
  author={ Jaderberg, Max  and  Simonyan, Karen  and  Zisserman, Andrew  and  Kavukcuoglu, Koray },
  journal={MIT Press},
  year={2015},
}"					images or feature map		feature map		"1. A CNN based spatial invariant module 
2. A differentiable sampling mechanism"				
																			
																			
26	Deep Residual Learning for Image Recognition					"@ARTICLE{2015arXiv151203385H,
       author = {{He}, Kaiming and {Zhang}, Xiangyu and {Ren}, Shaoqing and {Sun}, Jian},
        title = ""{Deep Residual Learning for Image Recognition}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = dec,
          eid = {arXiv:1512.03385},
        pages = {arXiv:1512.03385},
          doi = {10.48550/arXiv.1512.03385},
archivePrefix = {arXiv},
       eprint = {1512.03385},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151203385H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					images		classified labels, segmented images, etc.		An architecture for deep networks				
																			
																			
27	Rich feature hierarchies for accurate object detection and semantic segmentation					"@ARTICLE{2013arXiv1311.2524G,
       author = {{Girshick}, Ross and {Donahue}, Jeff and {Darrell}, Trevor and {Malik}, Jitendra},
        title = ""{Rich feature hierarchies for accurate object detection and semantic segmentation}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2013,
        month = nov,
          eid = {arXiv:1311.2524},
        pages = {arXiv:1311.2524},
          doi = {10.48550/arXiv.1311.2524},
archivePrefix = {arXiv},
       eprint = {1311.2524},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1311.2524G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					rgb		bounding box, labels		"1. A CNN based network for object detection
2. A new bounding box regression method"				
																			
																			
28	Fast R-CNN					"@ARTICLE{2015arXiv150408083G,
       author = {{Girshick}, Ross},
        title = ""{Fast R-CNN}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = apr,
          eid = {arXiv:1504.08083},
        pages = {arXiv:1504.08083},
          doi = {10.48550/arXiv.1504.08083},
archivePrefix = {arXiv},
       eprint = {1504.08083},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150408083G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					rgb		bounding box, labels		"1. A CNN based network for object detection
2. RoI pooling layer
3. A mini-batch sampling method for obtaining RoIs and their groundtruth
4. A multi-task loss function"				
																			
																			
29	"Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks"					"@ARTICLE{2015arXiv150601497R,
       author = {{Ren}, Shaoqing and {He}, Kaiming and {Girshick}, Ross and {Sun}, Jian},
        title = ""{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = jun,
          eid = {arXiv:1506.01497},
        pages = {arXiv:1506.01497},
          doi = {10.48550/arXiv.1506.01497},
archivePrefix = {arXiv},
       eprint = {1506.01497},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150601497R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					rgb		bounding box, labels		"1. A CNN based network for object detection
2. Region proposal network (anchor)
3. Three methods for training feature shared network
4. A 4-step training method"				
																			
																			
30	Mask R-CNN					"@ARTICLE{2017arXiv170306870H,
       author = {{He}, Kaiming and {Gkioxari}, Georgia and {Doll{\'a}r}, Piotr and {Girshick}, Ross},
        title = ""{Mask R-CNN}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.06870},
        pages = {arXiv:1703.06870},
          doi = {10.48550/arXiv.1703.06870},
archivePrefix = {arXiv},
       eprint = {1703.06870},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170306870H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					rgb		classified labels, segmented images, etc.		"1. A CNN based network for instance segmentation
2. RoIAlign for feature extraction
3. A new loss function"				
																			
																			
31	Grasp Proposal Networks: An End-to-End Solution for Visual Learning of Robotic Grasps					"@ARTICLE{2020arXiv200912606W,
       author = {{Wu}, Chaozheng and {Chen}, Jian and {Cao}, Qiaoyu and {Zhang}, Jianchi and {Tai}, Yunxin and {Sun}, Lin and {Jia}, Kui},
        title = ""{Grasp Proposal Networks: An End-to-End Solution for Visual Learning of Robotic Grasps}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2020,
        month = sep,
          eid = {arXiv:2009.12606},
        pages = {arXiv:2009.12606},
          doi = {10.48550/arXiv.2009.12606},
archivePrefix = {arXiv},
       eprint = {2009.12606},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200912606W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"													
																			
																			
32	"Graspness
discovery in clutters for fast and accurate grasp detection"					"@INPROCEEDINGS{9711469,
  author={Wang, Chenxi and Fang, Hao-Shu and Gou, Minghao and Fang, Hongjie and Gao, Jin and Lu, Cewu},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Graspness Discovery in Clutters for Fast and Accurate Grasp Detection}, 
  year={2021},
  volume={},
  number={},
  pages={15944-15953},
  doi={10.1109/ICCV48922.2021.01566}}
"													
																			
																			
33 *	"When Transformer Meets Robotic Grasping: Exploits
Context for Efficient Grasp Detection"					"@ARTICLE{9810182,
  author={Wang, Shaochen and Zhou, Zhangli and Kan, Zhen},
  journal={IEEE Robotics and Automation Letters}, 
  title={When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection}, 
  year={2022},
  volume={7},
  number={3},
  pages={8170-8177},
  doi={10.1109/LRA.2022.3187261}}
"					images		heat maps of grasp		"1. A transformer based network for grasp pose prediction
2. A new metric for grasp evaluation"				
																			
																			
34 *	"GQ-STN: Optimizing One-Shot Grasp Detection
based on Robustness Classifier"					"@INPROCEEDINGS{8967785,
  author={Gariépy, Alexandre and Ruel, Jean-Christophe and Chaib-draa, Brahim and Giguère, Philippe},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness Classifier}, 
  year={2019},
  volume={},
  number={},
  pages={3996-4003},
  doi={10.1109/IROS40897.2019.8967785}}
"					d		grasp pose, grasp robustness		"1. A transformer based network for grasp pose prediction and grasp robustness evaluation
2. A review of different grasping approaches
3. A new loss function"				
																			
																			
35	Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics					"@ARTICLE{2017arXiv170309312M,
       author = {{Mahler}, Jeffrey and {Liang}, Jacky and {Niyaz}, Sherdil and {Laskey}, Michael and {Doan}, Richard and {Liu}, Xinyu and {Aparicio Ojea}, Juan and {Goldberg}, Ken},
        title = ""{Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.09312},
        pages = {arXiv:1703.09312},
          doi = {10.48550/arXiv.1703.09312},
archivePrefix = {arXiv},
       eprint = {1703.09312},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170309312M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					d, proposed grasp		grasp robustness		"1. A CNN based network for grasp robustness evaluation
2. A dataset in simulated scenes
3. Some metrics for grasp quality evaluation"				
																			
																			
36	"A Novel Robotic Pushing and Grasping Method
Based on Vision Transformer and Convolution"					"@ARTICLE{10066254,
  author={Yu, Sheng and Zhai, Di-Hua and Xia, Yuanqing},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Novel Robotic Pushing and Grasping Method Based on Vision Transformer and Convolution}, 
  year={2023},
  volume={},
  number={},
  pages={1-14},
  doi={10.1109/TNNLS.2023.3244186}}
"					rgb and rgb-d		the position of objects after pushing, grasp position		"1. A novel push definition
2. A transformer based network for object position prediction after pushing
3. A CNN based network for grasp position prediction"				
																			
																			
37	"Learning Synergies between Pushing and Grasping
with Self-supervised Deep Reinforcement Learning"					"@INPROCEEDINGS{8593986,
  author={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning}, 
  year={2018},
  volume={},
  number={},
  pages={4238-4245},
  doi={10.1109/IROS.2018.8593986}}
"					rgb-d		push or grasp		A Q-learning based method for pushing and grasping tasks				
																			
																			
38	Grasp Pose Detection in Point Clouds					"@ARTICLE{2017arXiv170609911T,
       author = {{ten Pas}, Andreas and {Gualtieri}, Marcus and {Saenko}, Kate and {Platt}, Robert},
        title = ""{Grasp Pose Detection in Point Clouds}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.09911},
        pages = {arXiv:1706.09911},
          doi = {10.48550/arXiv.1706.09911},
archivePrefix = {arXiv},
       eprint = {1706.09911},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170609911T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		grasp pose, grasp quality		"1. A CNN based network for grasp pose estimation
2. A grasp representation
3. A dataset
4. A grasp sampling method
5. A grasp detection performance evaluation metric"				
																			
																			
39	Learning robust, real-time, reactive robotic grasping					"@article{doi:10.1177/0278364919859066,
author = {Douglas Morrison and Peter Corke and Jürgen Leitner},
title ={Learning robust, real-time, reactive robotic grasping},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {2-3},
pages = {183-201},
year = {2020},
doi = {10.1177/0278364919859066},
URL = {https://doi.org/10.1177/0278364919859066},
eprint = {https://doi.org/10.1177/0278364919859066},
abstract = { We present a novel approach to perform object-independent grasp synthesis from depth images via deep neural networks. Our generative grasping convolutional neural network (GG-CNN) predicts a pixel-wise grasp quality that can be deployed in closed-loop grasping scenarios. GG-CNN overcomes shortcomings in existing techniques, namely discrete sampling of grasp candidates and long computation times. The network is orders of magnitude smaller than other state-of-the-art approaches while achieving better performance, particularly in clutter. We run a suite of real-world tests, during which we achieve an 84\% grasp success rate on a set of previously unseen objects with adversarial geometry and 94\% on household items. The lightweight nature enables closed-loop control of up to 50 Hz, with which we observed 88\% grasp success on a set of household objects that are moved during the grasp attempt. We further propose a method combining our GG-CNN with a multi-view approach, which improves overall grasp success rate in clutter by 10\%. Code is provided at https://github.com/dougsm/ggcnn }
}
"					d		heat maps of grasp		"1. A novel grasp definition
2. A CNN based network for grasp prediction"				
																			
																			
40	SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again					"@ARTICLE{2017arXiv171110006K,
       author = {{Kehl}, Wadim and {Manhardt}, Fabian and {Tombari}, Federico and {Ilic}, Slobodan and {Navab}, Nassir},
        title = ""{SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = nov,
          eid = {arXiv:1711.10006},
        pages = {arXiv:1711.10006},
          doi = {10.48550/arXiv.1711.10006},
archivePrefix = {arXiv},
       eprint = {1711.10006},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171110006K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					rgb		bounding box, 6-DoF pose		"1. A CNN based network for object detection and 6-DoF pose prediction
2. A classification method replacing the pose regression
3. A new loss function
4. Pose refinement and verification methods
"				
																			
																			
41	Deep Hough Voting for 3D Object Detection in Point Clouds					"@ARTICLE{2019arXiv190409664Q,
       author = {{Qi}, Charles R. and {Litany}, Or and {He}, Kaiming and {Guibas}, Leonidas J.},
        title = ""{Deep Hough Voting for 3D Object Detection in Point Clouds}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = apr,
          eid = {arXiv:1904.09664},
        pages = {arXiv:1904.09664},
          doi = {10.48550/arXiv.1904.09664},
archivePrefix = {arXiv},
       eprint = {1904.09664},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190409664Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		bounding box, 6-DoF pose, classified label, etc.		"1. A CNN based network for object detection and 6-DoF pose prediction
2. A new loss function"				
																			
																			
42	"PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose
Estimation"					"@INPROCEEDINGS{9157414,
  author={He, Yisheng and Sun, Wei and Huang, Haibin and Liu, Jianran and Fan, Haoqiang and Sun, Jian},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation}, 
  year={2020},
  volume={},
  number={},
  pages={11629-11638},
  doi={10.1109/CVPR42600.2020.01165}}
"					rgb-d and point cloud		6-DoF grasp pose		A CNN based Hough voting network for 6-DoF pose estimation				
																			
																			
43	"RGB Matters: Learning 7-DoF Grasp Poses on
Monocular RGBD Images"					"@INPROCEEDINGS{9561409,
  author={Gou, Minghao and Fang, Hao-Shu and Zhu, Zhanda and Xu, Sheng and Wang, Chenxi and Lu, Cewu},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images}, 
  year={2021},
  volume={},
  number={},
  pages={13459-13466},
  doi={10.1109/ICRA48506.2021.9561409}}
"					rgb-d and point cloud		7-DoF grasp pose		"1. A CNN based two-staged network for 7-DoF grasp pose prediction
2. Angle-View Net and Angle-View heatmap
3. A filtering method for deriving valid gripper width and distance"				
																			
																			
44	ROI-based Robotic Grasp Detection for Object Overlapping Scenes					"@INPROCEEDINGS{8967869,
  author={Zhang, Hanbo and Lan, Xuguang and Bai, Site and Zhou, Xinwen and Tian, Zhiqiang and Zheng, Nanning},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={ROI-based Robotic Grasp Detection for Object Overlapping Scenes}, 
  year={2019},
  volume={},
  number={},
  pages={4768-4775},
  doi={10.1109/IROS40897.2019.8967869}}
"					rgb		5-DoF grasp box		"1. A CNN based network for grasp pose prediction
2. A dataset in cluttered scenes
3. A new loss function
4. Some metrics for grasp quality evaluation"				
																			
																			
45	High precision grasp pose detection in dense clutter					"@ARTICLE{2016arXiv160301564G,
       author = {{Gualtieri}, Marcus and {ten Pas}, Andreas and {Saenko}, Kate and {Platt}, Robert},
        title = ""{High precision grasp pose detection in dense clutter}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2016,
        month = mar,
          eid = {arXiv:1603.01564},
        pages = {arXiv:1603.01564},
          doi = {10.48550/arXiv.1603.01564},
archivePrefix = {arXiv},
       eprint = {1603.01564},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160301564G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		graspable score		"1. A CNN based network for grasp evaluation
2. A dataset
3. A new grasp representation
4. Some metrics for grasp quality evaluation"				
																			
																			
46	Constructing Force-Closure Grasps in 3D					"@article{1987Constructing,
  title={Constructing Force-Closure Grasps in 3D},
  author={ Nguyen, V. D. },
  journal={Proc. 1987 Intl. Conf. on Robotics and Automation},
  year={1987},
}"					n		n		1. Some theorems on how to determine force-closure grasps				
																			
																			
47	Swin Transformer: Hierarchical Vision Transformer using Shifted Windows					"
@ARTICLE{2021arXiv210314030L,
       author = {{Liu}, Ze and {Lin}, Yutong and {Cao}, Yue and {Hu}, Han and {Wei}, Yixuan and {Zhang}, Zheng and {Lin}, Stephen and {Guo}, Baining},
        title = ""{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = 2021,
        month = mar,
          eid = {arXiv:2103.14030},
        pages = {arXiv:2103.14030},
          doi = {10.48550/arXiv.2103.14030},
archivePrefix = {arXiv},
       eprint = {2103.14030},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210314030L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"					rgb		classified labels, segmented images, etc.		"1. A transformer based network for semantic segmentation and classification
2. Swin transformer block with regular and shifted windows
3. An efficient batch computation approach"				
																			
																			
48	6-DoF Contrastive Grasp Proposal Network					"@ARTICLE{2021arXiv210315995Z,
       author = {{Zhu}, Xinghao and {Sun}, Lingfeng and {Fan}, Yongxiang and {Tomizuka}, Masayoshi},
        title = ""{6-DoF Contrastive Grasp Proposal Network}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2021,
        month = mar,
          eid = {arXiv:2103.15995},
        pages = {arXiv:2103.15995},
          doi = {10.48550/arXiv.2103.15995},
archivePrefix = {arXiv},
       eprint = {2103.15995},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210315995Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					d		6-DoF grasp pose		"1. A CNN based network for 6-DoF grasp pose prediction
2. Some new loss functions
3. A dataset"				
																			
																			
49 *	MonoGraspNet: 6-DoF Grasping with a Single RGB Image					"@ARTICLE{2022arXiv220913036Z,
       author = {{Zhai}, Guangyao and {Huang}, Dianye and {Wu}, Shun-Cheng and {Jung}, Hyunjun and {Di}, Yan and {Manhardt}, Fabian and {Tombari}, Federico and {Navab}, Nassir and {Busam}, Benjamin},
        title = ""{MonoGraspNet: 6-DoF Grasping with a Single RGB Image}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
         year = 2022,
        month = sep,
          eid = {arXiv:2209.13036},
        pages = {arXiv:2209.13036},
          doi = {10.48550/arXiv.2209.13036},
archivePrefix = {arXiv},
       eprint = {2209.13036},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220913036Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					rgb		7-DoF grasp pose		"1. A CNN and transformer based network for grasp pose prediction
2. A dataset containing some transparent objects in cluttered scenes
3. A new grasp representation"				
																			
																			
50	6-DOF GraspNet: Variational Grasp Generation for Object Manipulation					"@ARTICLE{2019arXiv190510520M,
       author = {{Mousavian}, Arsalan and {Eppner}, Clemens and {Fox}, Dieter},
        title = ""{6-DOF GraspNet: Variational Grasp Generation for Object Manipulation}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
         year = 2019,
        month = may,
          eid = {arXiv:1905.10520},
        pages = {arXiv:1905.10520},
          doi = {10.48550/arXiv.1905.10520},
archivePrefix = {arXiv},
       eprint = {1905.10520},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190510520M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		6-DoF grasp pose, grasp quality score		"1. A variational autoencoder based generative model for grasp sampling
2. A grasp evaluation network
3. A grasp refinement method
4. A dataset
5. A new evaluation metric"				
																			
																			
51	6-DOF Grasping for Target-driven Object Manipulation in Clutter					"@ARTICLE{2019arXiv191203628M,
       author = {{Murali}, Adithyavairavan and {Mousavian}, Arsalan and {Eppner}, Clemens and {Paxton}, Chris and {Fox}, Dieter},
        title = ""{6-DOF Grasping for Target-driven Object Manipulation in Clutter}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = dec,
          eid = {arXiv:1912.03628},
        pages = {arXiv:1912.03628},
          doi = {10.48550/arXiv.1912.03628},
archivePrefix = {arXiv},
       eprint = {1912.03628},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191203628M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					rgb-d and point cloud		6-DoF grasp pose, grasp quality score, collision score		"1. A generative model for generating grasp candidates
2. A grasp evaluation network
3. A grasp refinement method
4. A collision detection network
5. A way of removing blocked objects to grasp the target object"				
																			
																			
52 *	End-to-End Learning to Grasp via Sampling from Object Point Clouds					"@ARTICLE{2022arXiv220305585A,
       author = {{Alliegro}, Antonio and {Rudorfer}, Martin and {Frattin}, Fabio and {Leonardis}, Ale{\v{s}} and {Tommasi}, Tatiana},
        title = ""{End-to-End Learning to Grasp via Sampling from Object Point Clouds}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2022,
        month = mar,
          eid = {arXiv:2203.05585},
        pages = {arXiv:2203.05585},
          doi = {10.48550/arXiv.2203.05585},
archivePrefix = {arXiv},
       eprint = {2203.05585},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220305585A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		7-DoF grasp pose, grasp quality score		"1. A CNN based network for grasp pose prediction
2. Two grasp representations
3. A contact point sampler
4. A multi-task loss function
5. A dataset of single-object scenes"				
																			
																			
53 *	Volumetric-based Contact Point Detection for 7-DoF Grasping					"@ARTICLE{2022arXiv220906675C,
       author = {{Cai}, Junhao and {Su}, Jingcheng and {Zhou}, Zida and {Cheng}, Hui and {Chen}, Qifeng and {Y Wang}, Michael},
        title = ""{Volumetric-based Contact Point Detection for 7-DoF Grasping}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics},
         year = 2022,
        month = sep,
          eid = {arXiv:2209.06675},
        pages = {arXiv:2209.06675},
          doi = {10.48550/arXiv.2209.06675},
archivePrefix = {arXiv},
       eprint = {2209.06675},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220906675C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					rgb-d and point cloud		7-DoF grasp pose, grasp quality score		"1. A CNN based network for grasp pose prediction
2. A grasp representation
3. A contact point pairs sampling method
4. A dataset of simulated multi-object scenes"				
																			
																			
54 *	Learning to Sample					"@ARTICLE{2018arXiv181201659D,
       author = {{Dovrat}, Oren and {Lang}, Itai and {Avidan}, Shai},
        title = ""{Learning to Sample}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2018,
        month = dec,
          eid = {arXiv:1812.01659},
        pages = {arXiv:1812.01659},
          doi = {10.48550/arXiv.1812.01659},
archivePrefix = {arXiv},
       eprint = {1812.01659},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181201659D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		sampled point cloud		"1. A CNN based network for sampling point cloud with certain or arbitrary size
2. A multi-task loss function"				
																			
																			
55 *	SampleNet: Differentiable Point Cloud Sampling					"@ARTICLE{2019arXiv191203663L,
       author = {{Lang}, Itai and {Manor}, Asaf and {Avidan}, Shai},
        title = ""{SampleNet: Differentiable Point Cloud Sampling}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2019,
        month = dec,
          eid = {arXiv:1912.03663},
        pages = {arXiv:1912.03663},
          doi = {10.48550/arXiv.1912.03663},
archivePrefix = {arXiv},
       eprint = {1912.03663},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191203663L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}"					point cloud		sampled point cloud		"1. A CNN based differentiable network for point cloud sampling
2. A soft projection method
3. A multi-task loss function"				
																			
																			
56	Automatic generation and detection of highly reliable fiducial markers under occlusion					"@article{GARRIDOJURADO20142280,
title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
journal = {Pattern Recognition},
volume = {47},
number = {6},
pages = {2280-2292},
year = {2014},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2014.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0031320314000235},
author = {S. Garrido-Jurado and R. Muñoz-Salinas and F.J. Madrid-Cuevas and M.J. Marín-Jiménez},
keywords = {Augmented reality, Fiducial marker, Computer vision},
abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.}
}"					images of markers		id, 6-DoF pose		A novel type of fiducial marker				
																			
																			
57	AprilTag: A robust and flexible visual fiducial system					"@INPROCEEDINGS{5979561,
  author={Olson, Edwin},
  booktitle={2011 IEEE International Conference on Robotics and Automation}, 
  title={AprilTag: A robust and flexible visual fiducial system}, 
  year={2011},
  volume={},
  number={},
  pages={3400-3407},
  doi={10.1109/ICRA.2011.5979561}}
"					images of markers		id, 6-DoF pose		A novel type of fiducial marker				
																			
																			
58	Determining and Improving the Localization Accuracy of AprilTag Detection					"@INPROCEEDINGS{9197427,
  author={Kallwies, Jan and Forkel, Bianca and Wuensche, Hans-Joachim},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Determining and Improving the Localization Accuracy of AprilTag Detection}, 
  year={2020},
  volume={},
  number={},
  pages={8288-8294},
  doi={10.1109/ICRA40945.2020.9197427}}
"					images of markers		refined images of markers		"1. A comparison between different types of markers
2. Corner refinement and edge refinement"				
																			
																			
59	"Deep Convolutional Neural Network for Image
Deconvolution"					"@inproceedings{2014Deep,
  title={Deep Convolutional Neural Network for Image Deconvolution},
  author={ Xu, L.  and  Ren, J. S. J.  and  Liu, C.  and  Jia, J. },
  booktitle={International Conference on Neural Information Processing Systems},
  pages={1790-1798},
  year={2014},
}"					images		deblurred images		"1. A CNN based network for image deblurring and denoising
2. The analysis of using CNN layers to approximate deconvolution"				
																			
																			
60	"Inpainting of Irregular Holes in a Manuscript 
using UNet and Partial Convolution"					"@INPROCEEDINGS{9182917,
  author={Kaur, Amreen and Raj, Ankit and Jayanthi, N. and Indu, S.},
  booktitle={2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Inpainting of Irregular Holes in a Manuscript using UNet and Partial Convolution}, 
  year={2020},
  volume={},
  number={},
  pages={778-784},
  doi={10.1109/ICIRCA48905.2020.9182917}}
"					images		inpainted images		"1. A CNN based U-Net shaped network for image inpainting
2. A dataset of damaged and non-damaged images
3. Partial convolutional layer
4. A new loss function"				
																			
																			
61																			
																			
																			
62																			
																			
																			
63																			
																			
																			
64																			
																			
																			
65	RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking					"@ARTICLE{2023arXiv230901918B,
       author = {{Bharadhwaj}, Homanga and {Vakil}, Jay and {Sharma}, Mohit and {Gupta}, Abhinav and {Tulsiani}, Shubham and {Kumar}, Vikash},
        title = ""{RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking}"",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Machine Learning},
         year = 2023,
        month = sep,
          eid = {arXiv:2309.01918},
        pages = {arXiv:2309.01918},
          doi = {10.48550/arXiv.2309.01918},
archivePrefix = {arXiv},
       eprint = {2309.01918},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230901918B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

"													
																			
																			
																			
																			
	Approaching vector, in-plane rotation, etc.																		
	Contact points																		
	Gripper closing area, occupied space, etc.																		
	Grasping box																		
	Grasping box + distance from image plane																		
![image](https://github.com/HenryWJL/Reference/assets/114377159/3bea3b46-d84b-43e8-bd71-9700d4f41d41)
